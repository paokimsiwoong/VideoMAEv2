{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import models\n",
    "from timm.models import create_model\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Identity()\n",
       "    (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (head_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (head): Linear(in_features=384, out_features=710, bias=True)\n",
       "  )\n",
       "  (1): Linear(in_features=710, out_features=1, bias=True)\n",
       "  (2): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone = create_model(\n",
    "    \"vit_small_patch16_224\",\n",
    "    img_size=224,\n",
    "    pretrained=False,\n",
    "    num_classes=710,\n",
    "    all_frames=16 * 8,\n",
    "    # tubelet_size=args.tubelet_size,\n",
    "    # drop_rate=args.drop,\n",
    "    # drop_path_rate=args.drop_path,\n",
    "    # attn_drop_rate=args.attn_drop_rate,\n",
    "    # head_drop_rate=args.head_drop_rate,\n",
    "    # drop_block_rate=None,\n",
    "    # use_mean_pooling=args.use_mean_pooling,\n",
    "    # init_scale=args.init_scale,\n",
    "    # with_cp=args.with_checkpoint,\n",
    ")\n",
    "\n",
    "load_dict = torch.load(\"/data/ephemeral/home/VideoMAEv2/pths/vit_s_k710_dl_from_giant.pth\")\n",
    "backbone.load_state_dict(load_dict[\"module\"])\n",
    "model = nn.Sequential(backbone, nn.Linear(710, 1), nn.Sigmoid())\n",
    "model.to(\"cuda\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> test_t.shape: torch.Size([1, 3, 128, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "test_t = torch.randn(1, 3, 16 * 8, 224, 224)\n",
    "print(f\"==>> test_t.shape: {test_t.shape}\")\n",
    "test_t = test_t.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> output.shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(test_t)\n",
    "    print(f\"==>> output.shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> output: 0.46524369716644287\n",
      "==>> output.shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "output = torch.squeeze(output)\n",
    "print(f\"==>> output: {output}\")\n",
    "print(f\"==>> output.shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> type(output.item()): <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"==>> type(output.item()): {type(output.item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(\n",
    "    \"vit_small_patch16_224\",\n",
    "    img_size=224,\n",
    "    pretrained=False,\n",
    "    num_classes=710,\n",
    "    all_frames=16*4,\n",
    "    # tubelet_size=args.tubelet_size,\n",
    "    # drop_rate=args.drop,\n",
    "    # drop_path_rate=args.drop_path,\n",
    "    # attn_drop_rate=args.attn_drop_rate,\n",
    "    # head_drop_rate=args.head_drop_rate,\n",
    "    # drop_block_rate=None,\n",
    "    # use_mean_pooling=args.use_mean_pooling,\n",
    "    # init_scale=args.init_scale,\n",
    "    # with_cp=args.with_checkpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> model: VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): Identity()\n",
      "  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "  (head_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=384, out_features=710, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"==>> model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> load_dict.keys(): dict_keys(['module'])\n"
     ]
    }
   ],
   "source": [
    "load_dict = torch.load(\"/data/ephemeral/home/VideoMAEv2/pths/vit_s_k710_dl_from_giant.pth\")\n",
    "print(f\"==>> load_dict.keys(): {load_dict.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(load_dict[\"module\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): Identity()\n",
       "  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=384, out_features=710, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> test_t.shape: torch.Size([1, 3, 64, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "test_t = torch.randn(1, 3, 16*4, 224, 224)\n",
    "print(f\"==>> test_t.shape: {test_t.shape}\")\n",
    "test_t = test_t.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): Identity()\n",
       "  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=384, out_features=710, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> output.shape: torch.Size([1, 710])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(test_t)\n",
    "    print(f\"==>> output.shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> test_tt.shape: torch.Size([1, 3, 64, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "test_tt = torch.randn(1, 3, 16 * 4, 224, 224)\n",
    "print(f\"==>> test_tt.shape: {test_tt.shape}\")\n",
    "test_tt = test_tt.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> output2.shape: torch.Size([1, 710])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output2 = model(test_tt)\n",
    "    print(f\"==>> output2.shape: {output2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> test_ttt.shape: torch.Size([1, 3, 64, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "test_ttt = torch.randn(1, 3, 16 * 4, 224, 224)\n",
    "print(f\"==>> test_ttt.shape: {test_ttt.shape}\")\n",
    "test_ttt = test_ttt.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> output3.shape: torch.Size([1, 710])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output3 = model(test_ttt)\n",
    "    print(f\"==>> output3.shape: {output3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> model: VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): Identity()\n",
      "  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "  (head_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=384, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.vit_small_patch16_224()\n",
    "print(f\"==>> model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> model: VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): Identity()\n",
      "  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "  (head_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=384, out_features=710, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.head = nn.Linear(384,710)\n",
    "print(f\"==>> model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> load_dict.keys(): dict_keys(['module'])\n"
     ]
    }
   ],
   "source": [
    "load_dict = torch.load(\"/data/ephemeral/home/VideoMAEv2/pths/vit_s_k710_dl_from_giant.pth\")\n",
    "print(f\"==>> load_dict.keys(): {load_dict.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(load_dict[\"module\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): Identity()\n",
       "  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=384, out_features=710, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> test_t.shape: torch.Size([4, 3, 16, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "test_t = torch.randn(4,3,16,224,224)\n",
    "print(f\"==>> test_t.shape: {test_t.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_t = test_t.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): Identity()\n",
       "  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=384, out_features=710, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> output.shape: torch.Size([4, 710])\n"
     ]
    }
   ],
   "source": [
    "output = model(test_t)\n",
    "print(f\"==>> output.shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
